# src/infer_style_seq2seq.py
"""
ê°ì • + ìƒí™© + ìŠ¤íƒ€ì¼ ê¸°ë°˜ Attention Seq2Seq ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸

- í•™ìŠµëœ ëª¨ë¸: models/style_seq2seq.pt
- ë‹¨ì–´ì¥: models/style_vocab.json
- êµ¬ì¡°:
    * train_style_seq2seq.py ì™€ ê°™ì€ EncoderRNN / DecoderRNN(LuongAttention) / Seq2Seq ì‚¬ìš©
    * n-gram ë°˜ë³µì„ ì¤„ì´ëŠ” ë””ì½”ë”© ì „ëµ ì ìš©
    * ë””ì½”ë”© ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ê±°ë‚˜ / ë°˜ë³µì´ ì‹¬í•˜ë©´
      -> "ëª¨ë¸ì´ ì¶œë ¥ì„ ì œëŒ€ë¡œ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤." ë©”ì‹œì§€ ì¶œë ¥

â€» ë°ëª¨ìš© í•˜ë“œì½”ë”© ë¬¸ì¥/ë£° ì—†ìŒ (ì „ë¶€ ëª¨ë¸ ê¸°ë°˜)
"""

import json
from pathlib import Path
from typing import Dict, List, Tuple

import torch
import torch.nn as nn

# ==============================
# 0. ê²½ë¡œ / í•˜ì´í¼íŒŒë¼ë¯¸í„°
# ==============================

MODEL_PATH: Path = Path("models/style_seq2seq.pt")
VOCAB_PATH: Path = Path("models/style_vocab.json")

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

SPECIAL_TOKENS = {
    "PAD": "<pad>",
    "BOS": "<bos>",
    "EOS": "<eos>",
    "UNK": "<unk>",
}

EMBED_SIZE = 256
HIDDEN_SIZE = 512
NUM_LAYERS = 1
MAX_DECODE_LEN = 60


# ==============================
# 1. Vocab & ìœ í‹¸
# ==============================

def load_vocab(path: Path) -> Tuple[Dict[str, int], Dict[int, str]]:
    with path.open(encoding="utf-8") as f:
        obj = json.load(f)
    token2idx: Dict[str, int] = obj["token2idx"]
    idx2token: Dict[int, str] = {idx: tok for tok, idx in token2idx.items()}
    return token2idx, idx2token


def tokenize(text: str) -> List[str]:
    return text.strip().split()


def encode_sentence(
    text: str,
    token2idx: Dict[str, int],
    add_bos: bool = True,
    add_eos: bool = True,
) -> List[int]:
    unk_idx = token2idx[SPECIAL_TOKENS["UNK"]]
    tokens = tokenize(text)
    ids: List[int] = []
    if add_bos:
        ids.append(token2idx[SPECIAL_TOKENS["BOS"]])
    for t in tokens:
        ids.append(token2idx.get(t, unk_idx))
    if add_eos:
        ids.append(token2idx[SPECIAL_TOKENS["EOS"]])
    return ids


def decode_indices(indices: List[int], idx2token: Dict[int, str]) -> str:
    tokens: List[str] = []
    for idx in indices:
        tok = idx2token.get(int(idx), SPECIAL_TOKENS["UNK"])
        if tok == SPECIAL_TOKENS["EOS"]:
            break
        if tok in (SPECIAL_TOKENS["BOS"], SPECIAL_TOKENS["PAD"]):
            continue
        tokens.append(tok)
    return " ".join(tokens).strip()


# ê°„ë‹¨í•œ ë°˜ë³µ ê°ì§€ í•¨ìˆ˜
def is_repetitive(text: str) -> bool:
    """
    í† í° ì¤‘ë³µ ì •ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ
    - í† í° ìˆ˜ê°€ ë„ˆë¬´ ì ê±°ë‚˜(<=1)
    - ìœ ë‹ˆí¬ í† í° ë¹„ìœ¨ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´(ì˜ˆ: 0.5 ë¯¸ë§Œ)
    'ë°˜ë³µì ì¸ ë¹„ì •ìƒ ì¶œë ¥'ìœ¼ë¡œ ê°„ì£¼
    """
    tokens = [t for t in tokenize(text) if t not in [".", ",", "!", "?", "â€¦"]]
    if len(tokens) <= 1:
        return True
    unique = set(tokens)
    ratio = len(unique) / len(tokens)
    return ratio < 0.5


# ==============================
# 2. ëª¨ë¸ êµ¬ì¡° (train ê³¼ ë™ì¼)
# ==============================

class EncoderRNN(nn.Module):
    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int,
                 pad_idx: int, num_layers: int = 1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)
        self.gru = nn.GRU(
            emb_size,
            hidden_size,
            num_layers=num_layers,
            bidirectional=False,
        )

    def forward(self, src: torch.Tensor):
        """
        src: (src_len, batch)
        """
        embedded = self.embedding(src)              # (src_len, B, D)
        outputs, hidden = self.gru(embedded)        # outputs: (src_len, B, H)
        return outputs, hidden                      # hidden: (num_layers, B, H)


class LuongAttention(nn.Module):
    """
    Luong dot-product attention:
    score(h_t, h_s) = h_t^T h_s
    """
    def __init__(self, hidden_size: int):
        super().__init__()
        self.hidden_size = hidden_size

    def forward(self, decoder_hidden: torch.Tensor, encoder_outputs: torch.Tensor):
        """
        decoder_hidden: (1, B, H)
        encoder_outputs: (src_len, B, H)
        return:
          context: (B, H)
          attn_weights: (B, src_len)
        """
        # (1, B, H) -> (B, 1, H)
        dec = decoder_hidden.permute(1, 0, 2)      # (B, 1, H)
        # encoder_outputs: (src_len, B, H) -> (B, src_len, H)
        enc = encoder_outputs.permute(1, 0, 2)     # (B, src_len, H)

        # dot-product: (B, 1, H) x (B, H, src_len) -> (B, 1, src_len)
        scores = torch.bmm(dec, enc.transpose(1, 2))   # (B, 1, src_len)
        attn_weights = torch.softmax(scores, dim=-1)   # (B, 1, src_len)

        # context: (B, 1, src_len) x (B, src_len, H) -> (B, 1, H)
        context = torch.bmm(attn_weights, enc)         # (B, 1, H)
        context = context.squeeze(1)                   # (B, H)
        attn_weights = attn_weights.squeeze(1)         # (B, src_len)

        return context, attn_weights


class DecoderRNN(nn.Module):
    """
    Luong Attention ê¸°ë°˜ Decoder:
      1) ì„ë² ë”© ì…ë ¥ìœ¼ë¡œ GRU í•œ ìŠ¤í…
      2) GRU output(h_t)ì™€ encoder_outputsì— ëŒ€í•´ attention ê³„ì‚°
      3) [h_t; context]ë¥¼ linear -> vocab ë¶„í¬
    """
    def __init__(self, vocab_size: int, emb_size: int, hidden_size: int,
                 pad_idx: int, num_layers: int = 1):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, emb_size, padding_idx=pad_idx)
        self.gru = nn.GRU(emb_size, hidden_size, num_layers=num_layers)
        self.attn = LuongAttention(hidden_size)
        self.fc = nn.Linear(hidden_size * 2, vocab_size)

    def forward(self,
                input_step: torch.Tensor,
                hidden: torch.Tensor,
                encoder_outputs: torch.Tensor):
        """
        input_step: (1, B)
        hidden:     (1, B, H)
        encoder_outputs: (src_len, B, H)
        """
        embedded = self.embedding(input_step)            # (1, B, D)
        output, hidden = self.gru(embedded, hidden)      # output: (1, B, H)

        # ì–´í…ì…˜
        context, attn_weights = self.attn(output, encoder_outputs)  # (B, H), (B, src_len)

        # output: (1, B, H) -> (B, H)
        output = output.squeeze(0)                       # (B, H)

        # [h_t; context] -> vocab
        concat = torch.cat([output, context], dim=1)     # (B, 2H)
        logits = self.fc(concat)                         # (B, vocab_size)

        return logits, hidden, attn_weights


class Seq2Seq(nn.Module):
    def __init__(self, encoder: EncoderRNN, decoder: DecoderRNN, pad_idx: int):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.pad_idx = pad_idx


# ==============================
# 3. ë””ì½”ë”© ì „ëµ (n-gram ë°˜ë³µ ì¤„ì´ê¸°)
# ==============================

@torch.no_grad()
def decode_with_strategy(
    model: Seq2Seq,
    src_text: str,
    token2idx: Dict[str, int],
    idx2token: Dict[int, str],
    device: torch.device,
    max_len: int = MAX_DECODE_LEN,
) -> str:
    """
    Attention Seq2Seq + ê°„ë‹¨í•œ n-gram ë°˜ë³µ ì–µì œ ë””ì½”ë”©
    (ëª¨ë¸ ì¶œë ¥ 100% ì‚¬ìš©, ê·œì¹™ì€ 'ì„ íƒ ê³¼ì •'ì—ì„œë§Œ ì‚¬ìš©)
    """
    model.eval()

    pad_idx = token2idx[SPECIAL_TOKENS["PAD"]]
    bos_idx = token2idx[SPECIAL_TOKENS["BOS"]]
    eos_idx = token2idx[SPECIAL_TOKENS["EOS"]]
    unk_idx = token2idx[SPECIAL_TOKENS["UNK"]]

    # 1) ì¸ì½”ë” ì…ë ¥ ì¤€ë¹„
    src_ids = encode_sentence(src_text, token2idx, add_bos=True, add_eos=True)
    src_tensor = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(1)  # (src_len, 1)

    encoder_outputs, hidden = model.encoder(src_tensor)

    # 2) ë””ì½”ë” ë°˜ë³µ
    input_step = torch.tensor([[bos_idx]], dtype=torch.long, device=device)
    generated: List[int] = []

    for step in range(max_len):
        logits, hidden, _ = model.decoder(input_step, hidden, encoder_outputs)
        # logits: (1, vocab_size)
        # top-kì—ì„œ í›„ë³´ë¥¼ ê³¨ë¼ ë°˜ë³µ/UNK/PAD/BOS ìµœëŒ€í•œ íšŒí”¼
        topv, topi = logits.topk(5, dim=1)  # (1, 5)

        chosen = None
        for cand in topi[0]:
            ci = cand.item()
            # EOSë¥¼ ë„ˆë¬´ ë¹¨ë¦¬ ë‚´ì§€ ì•Šë„ë¡ ìµœì†Œ ê¸¸ì´ ì˜ˆì‹œ (3 í† í°)
            if ci == eos_idx and len(generated) < 3:
                continue
            if ci in (pad_idx, bos_idx):
                continue

            # ê°„ë‹¨í•œ n-gram(2,3-gram) ë°˜ë³µ ë°©ì§€
            if len(generated) >= 2 and ci == generated[-1]:
                # ë°”ë¡œ ì§ì „ í† í° ë°˜ë³µì€ ì¼ë‹¨ í”¼í•¨
                continue
            if len(generated) >= 3 and generated[-2:] == [generated[-3], ci]:
                # a b a b ê°™ì€ íŒ¨í„´ë„ í”¼í•˜ê¸°
                continue

            chosen = ci
            break

        # ì „ë¶€ ê±¸ëŸ¬ì§€ë©´ ê·¸ëƒ¥ argmaxë¡œ
        if chosen is None:
            chosen = topi[0, 0].item()

        if chosen == eos_idx:
            break

        generated.append(chosen)
        input_step = torch.tensor([[chosen]], dtype=torch.long, device=device)

    return decode_indices(generated, idx2token)


# ==============================
# 4. ì¸í„°ë™í‹°ë¸Œ ë£¨í”„
# ==============================

def interactive_loop(model: Seq2Seq, token2idx: Dict[str, int], idx2token: Dict[int, str]):
    emo_map = {
        1: "E01",  # ê¸°ì¨
        2: "E02",  # ìŠ¬í””
        3: "E18",  # ë¶„ë…¸
        4: "E04",  # ë¶ˆì•ˆ
        5: "E05",  # ìƒì²˜
        6: "E06",  # ë‹¹í™©
    }

    ctx_map = {
        1: "daily",        # ì¼ìƒ
        2: "task",         # ì§„ë¡œ/í•™ì—…Â·ì—…ë¬´
        3: "relationship", # ê´€ê³„/ê°€ì¡±Â·ì—°ì• 
        4: "counsel",      # ê°ì •ìƒë‹´
    }

    style_map = {
        1: "í•´ìš”ì²´",
        2: "ë°˜ë§ì²´",
        3: "í•©ì‡¼ì²´",
    }

    print("\n=== ê°ì •/ìƒí™© ê¸°ë°˜ ë¬¸ì¥ ìŠ¤íƒ€ì¼ ë³€í™˜ ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ===")
    print("ë¬¸ì¥ì„ ì…ë ¥í•˜ë©´, ê°ì •/ìƒí™©/ë§íˆ¬ë¥¼ ê³¨ë¼ì„œ ë³€í™˜ ê²°ê³¼ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ ë¹ˆ ì¤„ì—ì„œ ì—”í„°ë¥¼ ëˆ„ë¥´ê±°ë‚˜ 'quit', 'q' ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    while True:
        src = input("ğŸ‘‰ ë³€í™˜í•  ì›ë¬¸ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš” : ").strip()
        if src == "" or src.lower() in ("quit", "q"):
            print("\n[ì¢…ë£Œ] ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ----- ê°ì • ì„ íƒ -----
        print("\n[ê°ì • ì„ íƒ]")
        print("  1. ê¸°ì¨")
        print("  2. ìŠ¬í””")
        print("  3. ë¶„ë…¸")
        print("  4. ë¶ˆì•ˆ")
        print("  5. ìƒì²˜")
        print("  6. ë‹¹í™©")
        emo_in = input("ë²ˆí˜¸ ì„ íƒ (ì—”í„°=1: ê¸°ì¨) : ").strip()
        emo_choice = 1 if emo_in == "" else max(1, min(6, int(emo_in)))
        emo_tag = emo_map[emo_choice]

        # ----- ìƒí™© ì„ íƒ -----
        print("\n[ìƒí™©/ì»¨í…ìŠ¤íŠ¸ ì„ íƒ]")
        print("  1. ì¼ìƒ")
        print("  2. ì§„ë¡œ/í•™ì—…Â·ì—…ë¬´")
        print("  3. ê´€ê³„/ê°€ì¡±Â·ì—°ì• ")
        print("  4. ê°ì •ìƒë‹´")
        ctx_in = input("ë²ˆí˜¸ ì„ íƒ (ì—”í„°=1: ì¼ìƒ) : ").strip()
        ctx_choice = 1 if ctx_in == "" else max(1, min(4, int(ctx_in)))
        ctx_tag = ctx_map[ctx_choice]

        # ----- ë§íˆ¬ ì„ íƒ -----
        print("\n[ë§íˆ¬(ìŠ¤íƒ€ì¼) ì„ íƒ]")
        print("  1. í•´ìš”ì²´")
        print("  2. ë°˜ë§ì²´")
        print("  3. í•©ì‡¼ì²´")
        style_in = input("ë²ˆí˜¸ ì„ íƒ (ì—”í„°=1: í•´ìš”ì²´) : ").strip()
        style_choice = 1 if style_in == "" else max(1, min(3, int(style_in)))
        style_tag = style_map[style_choice]

        # ----- ëª¨ë¸ ì…ë ¥ ë¬¸ì¥ êµ¬ì„± -----
        control_prefix = f"<ctx:{ctx_tag}> <emo:{emo_tag}> <style:{style_tag}>"
        model_input = f"{control_prefix} {src}".strip()

        print("\n[ëª¨ë¸ ì…ë ¥]")
        print(f"  {model_input}")
        print(f"  - ìƒí™©: {['ì¼ìƒ','ì§„ë¡œ/í•™ì—…Â·ì—…ë¬´','ê´€ê³„/ê°€ì¡±Â·ì—°ì• ','ê°ì •ìƒë‹´'][ctx_choice-1]}")
        print(f"  - ê°ì •: {['ê¸°ì¨','ìŠ¬í””','ë¶„ë…¸','ë¶ˆì•ˆ','ìƒì²˜','ë‹¹í™©'][emo_choice-1]}")
        print(f"  - ë§íˆ¬: {style_tag}")

        # ----- ë””ì½”ë”© -----
        raw_out = decode_with_strategy(model, model_input, token2idx, idx2token, DEVICE)

        print("\nğŸ§  ëª¨ë¸ ì¶œë ¥ :", raw_out if raw_out else "(ë¹ˆ ë¬¸ì¥)")

        # ë°˜ë³µ/ë¹„ì •ìƒ ì—¬ë¶€ íŒë‹¨
        if (not raw_out) or is_repetitive(raw_out):
            print("âš ï¸ ëª¨ë¸ì´ ì¶œë ¥ì„ ì œëŒ€ë¡œ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (ë°˜ë³µ ë˜ëŠ” ë¹„ì •ìƒ ì¶œë ¥ìœ¼ë¡œ íŒë‹¨)\n")
        else:
            print("âœ¨ ìµœì¢… ë³€í™˜ ê²°ê³¼ :", raw_out, "\n")


# ==============================
# 5. main
# ==============================

def main():
    print(f"âœ… ëª¨ë¸ ë¡œë“œ ì‹œë„: {MODEL_PATH}")
    print(f"âœ… ë‹¨ì–´ì¥ ë¡œë“œ ì‹œë„: {VOCAB_PATH}")

    if not MODEL_PATH.exists():
        raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {MODEL_PATH}")
    if not VOCAB_PATH.exists():
        raise FileNotFoundError(f"ë‹¨ì–´ì¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {VOCAB_PATH}")

    token2idx, idx2token = load_vocab(VOCAB_PATH)
    vocab_size = len(token2idx)
    print(f"âœ… ë‹¨ì–´ì¥ ë¡œë“œ ì™„ë£Œ (vocab_size={vocab_size})")

    pad_idx = token2idx[SPECIAL_TOKENS["PAD"]]

    encoder = EncoderRNN(vocab_size, EMBED_SIZE, HIDDEN_SIZE, pad_idx, NUM_LAYERS)
    decoder = DecoderRNN(vocab_size, EMBED_SIZE, HIDDEN_SIZE, pad_idx, NUM_LAYERS)
    model = Seq2Seq(encoder, decoder, pad_idx).to(DEVICE)

    state = torch.load(MODEL_PATH, map_location=DEVICE)
    model.load_state_dict(state)
    print("âœ… Attention Seq2Seq ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    interactive_loop(model, token2idx, idx2token)


if __name__ == "__main__":
    main()
